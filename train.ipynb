{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import string\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "label2id = {\n",
    "    '0': 0, '[name]': 1, '[name_1]': 1, '[name_2]': 1, '[scientist-name]': 1,\n",
    "    '[surname]': 2, '[surname_1]': 2, '[surname_2]': 2, '[age]': 3,\n",
    "    '[date-of-birth]': 4, '[date]': 5, '[sex]': 6, '[religion]': 7,\n",
    "    '[political-view]': 8, '[#political-view]': 8, '[ethnicity]': 9,\n",
    "    '[sexual-orientation]': 10, '[health]': 11, '[#health]': 11,\n",
    "    '[weight-loss/muscle-gain]': 11, '[vegetarian/vegan]': 11,\n",
    "    '[active/sedentary]': 11, '[relative]': 12, '[city]': 13, '[#city]': 13,\n",
    "    '[address]': 14, '[street]': 14, '[email]': 15, '[phone]': 16,\n",
    "    '[pesel]': 17, '[document-number]': 18, '[ID number]': 18, '[vin]': 18,\n",
    "    '[company]': 19, '[school-name]': 20, '[job-title]': 21,\n",
    "    '[healthcare-professional]': 21, '[bank-account]': 22,\n",
    "    '[credit-card-number]': 23, '[username]': 24, '[social-media-username]': 24,\n",
    "    '[secret]': 25\n",
    "}\n",
    "\n",
    "id2label = {\n",
    "    0: '0',\n",
    "    1: '[name]',\n",
    "    2: '[surname]',\n",
    "    3: '[age]',\n",
    "    4: '[date-of-birth]',\n",
    "    5: '[date]',\n",
    "    6: '[sex]',\n",
    "    7: '[religion]',\n",
    "    8: '[political-view]',\n",
    "    9: '[ethnicity]',\n",
    "    10: '[sexual-orientation]',\n",
    "    11: '[health]',\n",
    "    12: '[relative]',\n",
    "    13: '[city]',\n",
    "    14: '[address]',\n",
    "    15: '[email]',\n",
    "    16: '[phone]',\n",
    "    17: '[pesel]',\n",
    "    18: '[document-number]',\n",
    "    19: '[company]',\n",
    "    20: '[school-name]',\n",
    "    21: '[job-title]',\n",
    "    22: '[bank-account]',\n",
    "    23: '[credit-card-number]',\n",
    "    24: '[username]',\n",
    "    25: '[secret]'\n",
    "}\n",
    "\n",
    "RAW_FILE = Path(\"anonymized.txt\")\n",
    "ANNOTATED_FILE = Path(\"orig.txt\")\n",
    "\n",
    "def clean_token(token: str) -> str:\n",
    "    return token.strip(string. punctuation)\n",
    "\n",
    "def parse_alignment(raw_str: str, anno_str: str) -> tuple[list[str], list[int]]:\n",
    "    raw_tokens = raw_str.split()\n",
    "    anno_tokens = anno_str.split()\n",
    "    tags = []\n",
    "\n",
    "    r_idx, a_idx = 0, 0\n",
    "    OUTSIDE_ID = label2id['0']\n",
    "\n",
    "    while r_idx < len(raw_tokens):\n",
    "        if a_idx >= len(anno_tokens):\n",
    "            last_anno = clean_token(anno_tokens[-1]) if anno_tokens else None\n",
    "            tags.append(label2id. get(last_anno, OUTSIDE_ID) if last_anno else OUTSIDE_ID)\n",
    "            r_idx += 1\n",
    "            continue\n",
    "\n",
    "        raw_word = raw_tokens[r_idx]\n",
    "        anno_word = anno_tokens[a_idx]\n",
    "        cleaned_anno = clean_token(anno_word)\n",
    "        is_tag = cleaned_anno in label2id and cleaned_anno != '0'\n",
    "\n",
    "        if raw_word == anno_word and not is_tag:\n",
    "            tags.append(OUTSIDE_ID)\n",
    "            r_idx += 1\n",
    "            a_idx += 1\n",
    "        elif is_tag:\n",
    "            tag_id = label2id[cleaned_anno]\n",
    "            next_anchor = anno_tokens[a_idx + 1] if a_idx + 1 < len(anno_tokens) else None\n",
    "            next_is_tag = next_anchor is not None and clean_token(next_anchor) in label2id\n",
    "\n",
    "            if next_anchor and not next_is_tag:\n",
    "                start_r = r_idx\n",
    "                start_tags_len = len(tags)\n",
    "                found_anchor = False\n",
    "\n",
    "                while r_idx < len(raw_tokens):\n",
    "                    if clean_token(raw_tokens[r_idx]) == clean_token(next_anchor):\n",
    "                        found_anchor = True\n",
    "                        break\n",
    "                    tags.append(tag_id)\n",
    "                    r_idx += 1\n",
    "\n",
    "                if not found_anchor:\n",
    "                    tags = tags[:start_tags_len]\n",
    "                    tags.append(tag_id)\n",
    "                    r_idx = start_r + 1\n",
    "\n",
    "                a_idx += 1\n",
    "            else:\n",
    "                tags.append(tag_id)\n",
    "                r_idx += 1\n",
    "                a_idx += 1\n",
    "        else:\n",
    "            tags.append(OUTSIDE_ID)\n",
    "            r_idx += 1\n",
    "            a_idx += 1\n",
    "\n",
    "    return raw_tokens, tags\n",
    "\n",
    "def load_dataset(raw_path: Path, anno_path: Path) -> Dataset:\n",
    "    with open(raw_path, encoding=\"utf-8\") as f:\n",
    "        raw_lines = f.read().splitlines()\n",
    "    with open(anno_path, encoding=\"utf-8\") as f:\n",
    "        annotated_lines = f.read().splitlines()\n",
    "\n",
    "    if len(raw_lines) != len(annotated_lines):\n",
    "        print(f\"WARNING: Line count mismatch - raw: {len(raw_lines)}, annotated: {len(annotated_lines)}\")\n",
    "\n",
    "    formatted_data = []\n",
    "    mismatches = 0\n",
    "\n",
    "    for i, (raw, anno) in enumerate(zip(raw_lines, annotated_lines)):\n",
    "        if not raw.strip() or not anno.strip():\n",
    "            continue\n",
    "\n",
    "        tokens, ner_tags = parse_alignment(raw, anno)\n",
    "\n",
    "        if len(tokens) != len(ner_tags):\n",
    "            mismatches += 1\n",
    "            m = min(len(tokens), len(ner_tags))\n",
    "            tokens, ner_tags = tokens[:m], ner_tags[:m]\n",
    "\n",
    "        formatted_data.append({\"tokens\": tokens, \"ner_tags\": ner_tags})\n",
    "\n",
    "    if mismatches:\n",
    "        print(f\"WARNING: {mismatches} rows had length mismatches and were truncated\")\n",
    "\n",
    "    return Dataset.from_list(formatted_data)\n",
    "\n",
    "dataset = load_dataset(RAW_FILE, ANNOTATED_FILE)\n",
    "print(f\"Dataset created with {len(dataset)} rows.\")\n"
   ],
   "id": "c610bdc42b05050b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._loss_fct = None\n",
    "        self._num_labels = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        num_labels = logits.shape[-1]\n",
    "\n",
    "        if self._loss_fct is None or self._num_labels != num_labels:\n",
    "            weight_tensor = torch.tensor(\n",
    "                [1.0] + [5.0] * (num_labels - 1),\n",
    "                device=model.device,\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "            self._loss_fct = nn.CrossEntropyLoss(weight=weight_tensor)\n",
    "            self._num_labels = num_labels\n",
    "\n",
    "        if labels is not None:\n",
    "            active_loss = inputs[\"attention_mask\"].view(-1) == 1\n",
    "            active_logits = logits. view(-1, num_labels)\n",
    "            active_labels = torch.where(\n",
    "                active_loss,\n",
    "                labels.view(-1),\n",
    "                torch.tensor(self._loss_fct.ignore_index). type_as(labels)\n",
    "            )\n",
    "            loss = self._loss_fct(active_logits, active_labels)\n",
    "        else:\n",
    "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "MODEL_CHECKPOINT = \"allegro/herbert-large-cased\"\n",
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "train_test_split = tokenized_datasets.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"nerbert\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=True,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"f1\"\n",
    ")\n",
    "\n",
    "trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "print(\"Starting training\")\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"nerbert\")\n",
    "tokenizer.save_pretrained(\"nerbert\")\n",
    "print(\"Model saved to 'nerbert'\")"
   ],
   "id": "131b78a3d9d61583"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
